{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECES 641 Grad Project\n",
    "# Phil Huddy\n",
    "# Raymond Yung\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import Entrez, SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "PHIL_EMAIL = \"pdh46@drexel.edu\"\n",
    "RAYMOND_EMAIL = \"\"\n",
    "\n",
    "Entrez.email = PHIL_EMAIL\n",
    "\n",
    "# read in the aligned sequences\n",
    "aligned_sequences = []\n",
    "for seq in SeqIO.parse(\"output.mafft\", \"fasta\"):\n",
    "    aligned_sequences.append(seq)\n",
    "\n",
    "# get rid of reference sequence\n",
    "aligned_sequences.pop(0)\n",
    "\n",
    "# load in ISM annotation info\n",
    "ism_annotation = pd.read_csv('ISM_annotation.txt', sep=',')\n",
    "\n",
    "# create an empty array to store ISMs\n",
    "ism_data = []\n",
    "\n",
    "# generate the ISMs by grabbing the nucleotide at each Ref position in ISM_annotation.txt\n",
    "for aligned_seq in aligned_sequences:\n",
    "    cur_ism = ''\n",
    "    for pos in ism_annotation['Ref position']:\n",
    "        cur_ism += aligned_seq.seq[(pos-1)]\n",
    "\n",
    "    ism_data.append([aligned_seq.name, cur_ism])\n",
    "\n",
    "isms = pd.DataFrame(ism_data, columns=[\"name\", \"seq\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What subtype is each sample assigned to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                name                                seq\n",
      "0    5_19_S_2.sorted  nnaacttcggtccgcaccctaggncngctcggg\n",
      "1      5_19_S.sorted  tnaacttcggtccgcaccctagggcngntcggg\n",
      "2      5_28_S.sorted  tcaacttcgnnccgcaccctagggcngctcggg\n",
      "3      6_09_S.sorted  tcaacttcggnccgcaccctagggcggctcnnn\n",
      "4  6_30_S_COL.sorted  ttaacttcggtccgcaccctaggnctgctcggg\n",
      "5  6_30_S_MOS.sorted  ntaacttcggtccgcaccctagggctgntcggg\n",
      "6  MR_7_1_MOS.sorted  tnaacttcggtccgcaccctaggnctgctcggg\n"
     ]
    }
   ],
   "source": [
    "print(isms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is the quality of the ISMs, do you see any ambiguous bases? If so, why is that?  \n",
    "\n",
    "- Amgbigious Bases are defined as bases with missing protein markers in them. As Zhao et Al explains, it is the ambiguities in reported sequence data. \n",
    "    - **\"Bases like N and - represent a fully amigous site and gap respectivel are substantially less informative.\"**\n",
    "         - -Zhao et Al\n",
    "         \n",
    "- Yes we do see ambigious bases. For the most part, the quality of our ISM's are pretty good even though there are a few gaps given by the n bases shown in our sequences. As shown below, the ambiguity represnts at most, 12.12% of our total sequences. The wastewater papers in which these samples are associated with states that the \n",
    "    - \"SARS-CoV-2 was not strongly correlated with RT-qPCR genome copy quantification, which is **likely due to the variability intruduced by different extraction methods**.\" \n",
    "    - Furthermore, \"Only samples with RT-qPCR CT-values <33 (~25 gc/uL) yielded complete consensus genomes\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_19_S_2.sorted has 4 ambigious bases, which is 12.121212121212121 of its total sequence\n",
      "5_19_S.sorted has 3 ambigious bases, which is 9.090909090909092 of its total sequence\n",
      "5_28_S.sorted has 3 ambigious bases, which is 9.090909090909092 of its total sequence\n",
      "6_09_S.sorted has 4 ambigious bases, which is 12.121212121212121 of its total sequence\n",
      "6_30_S_COL.sorted has 1 ambigious bases, which is 3.0303030303030303 of its total sequence\n",
      "6_30_S_MOS.sorted has 2 ambigious bases, which is 6.0606060606060606 of its total sequence\n",
      "MR_7_1_MOS.sorted has 2 ambigious bases, which is 6.0606060606060606 of its total sequence\n"
     ]
    }
   ],
   "source": [
    "data_length = len(isms['name'])\n",
    "for seq in range(data_length):\n",
    "    n_count = isms['seq'][seq].count('n');\n",
    "    seq_length = len(isms['seq'][seq])\n",
    "    print(isms['name'][seq],'has', n_count, 'ambigious bases, which is', n_count/seq_length*100, 'of its total sequence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find the consensus ISM of these 7 ISMs?\n",
    "- \"Aside from the from ambigious bases are any of the other positions matching? And can you replace the ambigious bases with those bases based off that consesus\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(word):\n",
    "    return np.asarray([char for char in word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "isms_modified = [];\n",
    "\n",
    "for seq in range(data_length):\n",
    "    isms_modified.append(split(isms['seq'][seq]))\n",
    "    \n",
    "isms_modified = np.asarray(isms_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections as collections\n",
    "isms_modified = np.transpose(isms_modified) #Transpose so we can view columns instead. Easier to iterate imo\n",
    "[L, W] = np.shape(isms_modified) #Can iterate this L wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      t    g    c    a    n\n",
      "0   5.0  0.0  0.0  0.0  2.0\n",
      "1   2.0  0.0  2.0  0.0  3.0\n",
      "2   0.0  0.0  0.0  7.0  0.0\n",
      "3   0.0  0.0  0.0  7.0  0.0\n",
      "4   0.0  0.0  7.0  0.0  0.0\n",
      "5   7.0  0.0  0.0  0.0  0.0\n",
      "6   7.0  0.0  0.0  0.0  0.0\n",
      "7   0.0  0.0  7.0  0.0  0.0\n",
      "8   0.0  7.0  0.0  0.0  0.0\n",
      "9   0.0  6.0  0.0  0.0  1.0\n",
      "10  5.0  0.0  0.0  0.0  2.0\n",
      "11  0.0  0.0  7.0  0.0  0.0\n",
      "12  0.0  0.0  7.0  0.0  0.0\n",
      "13  0.0  7.0  0.0  0.0  0.0\n",
      "14  0.0  0.0  7.0  0.0  0.0\n",
      "15  0.0  0.0  0.0  7.0  0.0\n",
      "16  0.0  0.0  7.0  0.0  0.0\n",
      "17  0.0  0.0  7.0  0.0  0.0\n",
      "18  0.0  0.0  7.0  0.0  0.0\n",
      "19  7.0  0.0  0.0  0.0  0.0\n",
      "20  0.0  0.0  0.0  7.0  0.0\n",
      "21  0.0  7.0  0.0  0.0  0.0\n",
      "22  0.0  7.0  0.0  0.0  0.0\n",
      "23  0.0  4.0  0.0  0.0  3.0\n",
      "24  0.0  0.0  7.0  0.0  0.0\n",
      "25  3.0  1.0  0.0  0.0  3.0\n",
      "26  0.0  7.0  0.0  0.0  0.0\n",
      "27  0.0  0.0  5.0  0.0  2.0\n",
      "28  7.0  0.0  0.0  0.0  0.0\n",
      "29  0.0  0.0  7.0  0.0  0.0\n",
      "30  0.0  6.0  0.0  0.0  1.0\n",
      "31  0.0  6.0  0.0  0.0  1.0\n",
      "32  0.0  6.0  0.0  0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "base_list = list('tgcan');\n",
    "hits = pd.DataFrame(columns = base_list, index = None)\n",
    "\n",
    "for i in range(L):\n",
    "    collected = collections.Counter(isms_modified[i])\n",
    "    hit_array = np.zeros(len(base_list))\n",
    "    for base, count in collected.items():\n",
    "        index = base_list.index(base)\n",
    "        hit_array[index] = count\n",
    "    hit_array = pd.DataFrame([hit_array], columns = base_list, index = [i])\n",
    "    hits = hits.append(hit_array)\n",
    "    \n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation for what's going on here: \n",
    "\n",
    "I transposd the sequences so that we can view all the bases per position and counted the number of repeated hits per position. Now we can view the ambiguity by seeing all the unique bases that only have 1 consensus base. We can create a mediocre confidence value by also dividing the total value of the consesus at the posisition by the total number of sequences (in our case 7).    \n",
    "  \n",
    "Building onto this, we can do this for all the possible bases at that position, but really we only want a confidence level that's greater that 70% in my opinion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(lvl, df):\n",
    "    hits_confidence = df.copy()\n",
    "    index = np.where(hits_confidence.div(7) >= lvl)\n",
    "    row, col = np.shape(index)\n",
    "\n",
    "    consensus = []\n",
    "    for i in range(col):\n",
    "        base = hits_confidence.columns[index[1][i]]\n",
    "        if base != 'n':\n",
    "            position = index[0][i]\n",
    "            count = hits_confidence.iloc[index[0][i]][index[1][i]]\n",
    "            ncount = hits_confidence.iloc[index[0][i]][4]\n",
    "            confidence = count/(7-ncount)\n",
    "            consensus.append([base, position, count, ncount, confidence])\n",
    "        \n",
    "    consensus = pd.DataFrame(consensus, columns=[\"base\", \"position\",\"count\",\"ncount\",\"confidence\"])\n",
    "    return consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base  position  count  ncount  confidence\n",
      "   t         0    5.0     2.0        1.00\n",
      "   t         1    2.0     3.0        0.50\n",
      "   c         1    2.0     3.0        0.50\n",
      "   a         2    7.0     0.0        1.00\n",
      "   a         3    7.0     0.0        1.00\n",
      "   c         4    7.0     0.0        1.00\n",
      "   t         5    7.0     0.0        1.00\n",
      "   t         6    7.0     0.0        1.00\n",
      "   c         7    7.0     0.0        1.00\n",
      "   g         8    7.0     0.0        1.00\n",
      "   g         9    6.0     1.0        1.00\n",
      "   t        10    5.0     2.0        1.00\n",
      "   c        11    7.0     0.0        1.00\n",
      "   c        12    7.0     0.0        1.00\n",
      "   g        13    7.0     0.0        1.00\n",
      "   c        14    7.0     0.0        1.00\n",
      "   a        15    7.0     0.0        1.00\n",
      "   c        16    7.0     0.0        1.00\n",
      "   c        17    7.0     0.0        1.00\n",
      "   c        18    7.0     0.0        1.00\n",
      "   t        19    7.0     0.0        1.00\n",
      "   a        20    7.0     0.0        1.00\n",
      "   g        21    7.0     0.0        1.00\n",
      "   g        22    7.0     0.0        1.00\n",
      "   g        23    4.0     3.0        1.00\n",
      "   c        24    7.0     0.0        1.00\n",
      "   t        25    3.0     3.0        0.75\n",
      "   g        25    1.0     3.0        0.25\n",
      "   g        26    7.0     0.0        1.00\n",
      "   c        27    5.0     2.0        1.00\n",
      "   t        28    7.0     0.0        1.00\n",
      "   c        29    7.0     0.0        1.00\n",
      "   g        30    6.0     1.0        1.00\n",
      "   g        31    6.0     1.0        1.00\n",
      "   g        32    6.0     1.0        1.00\n"
     ]
    }
   ],
   "source": [
    "test = confidence(.1, hits)\n",
    "print(test.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "def convert_to_sequence(df_mat):\n",
    "    position = df_mat['position'].values.flatten()\n",
    "    con_index = []\n",
    "    for k, g in groupby(enumerate(position), lambda x:x[0]-x[1]):\n",
    "        con_index.append(list(map(itemgetter(1), g)))\n",
    "    consensus = [];\n",
    "    confidence = [1];\n",
    "    test = np.concatenate(con_index)\n",
    "    \n",
    "    if len(np.unique(test)) > np.max(position):\n",
    "        con_index = np.unique(test)\n",
    "        b_name = []\n",
    "        for i in con_index:\n",
    "            p_count = np.where(df_mat['position']==i)[0]\n",
    "            \n",
    "            if len(p_count) == 1:\n",
    "                if len(b_name) <= 1:\n",
    "                    b_name.append(df_mat['base'][int(p_count)])\n",
    "                    b_name = ''.join(b_name)\n",
    "                    confidence[0] = confidence[0]*df_mat['confidence'][int(p_count)]\n",
    "                    \n",
    "                else:\n",
    "                    for i in range(len(b_name)):\n",
    "                        b_name[i] = [b_name[i], df_mat['base'][int(p_count)]]\n",
    "                        #print(confidence[i])\n",
    "                        confidence[i] = confidence[i]*df_mat['confidence'][int(p_count)]\n",
    "                        b_name[i] = ''.join(b_name[i])\n",
    "            else:\n",
    "                n_name = []\n",
    "                con_1 = []\n",
    "                for b in range(len(b_name)):\n",
    "                    for p in p_count:\n",
    "                        t_con = confidence[b]*df_mat['confidence'][p]\n",
    "                        temp = [b_name[b], df_mat['base'][p]] \n",
    "                        n_name.append(''.join(temp))\n",
    "                        con_1.append(t_con)\n",
    "                b_name = n_name\n",
    "                confidence = con_1\n",
    "       \n",
    "        return(b_name, confidence)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        for p in con_index:\n",
    "            base_name = [];\n",
    "            con = [];\n",
    "            print(p)\n",
    "            for b_num in p:\n",
    "                cur_idx = np.where(df_mat['position']==b_num)[0]\n",
    "                for idx in cur_idx:\n",
    "                    base_name.append((df_mat['base'][idx]))\n",
    "                    con.append((df_mat['confidence'][idx]))\n",
    "            consensus.append(''.join(base_name))\n",
    "        \n",
    "        return(consensus, con_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ttaacttcggtccgcaccctagggctgctcggg 0.375\n",
      "ttaacttcggtccgcaccctagggcggctcggg 0.125\n",
      "tcaacttcggtccgcaccctagggctgctcggg 0.375\n",
      "tcaacttcggtccgcaccctagggcggctcggg 0.125\n"
     ]
    }
   ],
   "source": [
    "thing = convert_to_sequence(test)\n",
    "for num in range(len(thing[0])):\n",
    "    print(thing[0][num], thing[1][num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.argsort(thing[1])\n",
    "a = np.asarray(thing[0])\n",
    "b = np.asarray(thing[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_thing = a[temp[::-1]]\n",
    "new_thing_val = b[temp[::-1]]\n",
    "thing =(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcaacttcggtccgcaccctagggctgctcggg 0.375\n",
      "ttaacttcggtccgcaccctagggctgctcggg 0.375\n",
      "tcaacttcggtccgcaccctagggcggctcggg 0.125\n",
      "ttaacttcggtccgcaccctagggcggctcggg 0.125\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(new_thing)):\n",
    "    print(new_thing[i], new_thing_val[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lev_string(word1, word2):\n",
    "    #Get sizes of words... Will need to create matrix\n",
    "    #Making sure that s1 is the largest\n",
    "    if len(word1) >= len(word2):\n",
    "        large = len(word1); small = len(word2)\n",
    "        l_word = word1; s_word = word2;\n",
    "    else:\n",
    "        large = len(word2); small = len(word1)\n",
    "        l_word = word2; s_word = word1;\n",
    "        \n",
    "    #For Consistency... Keep Largest on Horizontal and Smallest on Vertical\n",
    "    distance = np.zeros((small+1, large+1))\n",
    "    distance[0,:] = np.array([i for i in range(large+1)])\n",
    "    distance[:,0] = np.array([i for i in range(small+1)])\n",
    "    \n",
    "    for col in range(1, large+1):\n",
    "        for row in range(1, small+1):\n",
    "            #Only add 1 if the characters at word1[i] not equal word2[j]\n",
    "            if (l_word[col-1] == s_word[row-1]):\n",
    "                distance[row, col] = distance[row-1, col-1]\n",
    "            else:\n",
    "                # Piecewise If Statements\n",
    "                a1 = distance[row-1, col] + 1 \n",
    "                a2 = distance[row, col-1] + 1\n",
    "                a3 = distance[row-1, col-1] + 1\n",
    "                distance[row, col] = min(a1,a2,a3)\n",
    "    return(distance, distance[-1,-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mak = 'TTAACTTCGGTCCGCACCCTAGGGCGGCTCGGG'\n",
    "mak = mak.lower()\n",
    "lev_string(mak, new_thing[3])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>closest</th>\n",
       "      <th>distance</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nnaacttcggtccgcaccctaggncngctcggg</td>\n",
       "      <td>ttaacttcggtccgcaccctagggctgctcggg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5_19_S_2.sorted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tnaacttcggtccgcaccctagggcngntcggg</td>\n",
       "      <td>ttaacttcggtccgcaccctagggctgctcggg</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5_19_S.sorted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tcaacttcgnnccgcaccctagggcngctcggg</td>\n",
       "      <td>ttaacttcggtccgcaccctagggctgctcggg</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5_28_S.sorted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tcaacttcggnccgcaccctagggcggctcnnn</td>\n",
       "      <td>tcaacttcggtccgcaccctagggctgctcggg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6_09_S.sorted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ttaacttcggtccgcaccctaggnctgctcggg</td>\n",
       "      <td>ttaacttcggtccgcaccctagggcggctcggg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6_30_S_COL.sorted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ntaacttcggtccgcaccctagggctgntcggg</td>\n",
       "      <td>ttaacttcggtccgcaccctagggcggctcggg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6_30_S_MOS.sorted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tnaacttcggtccgcaccctaggnctgctcggg</td>\n",
       "      <td>ttaacttcggtccgcaccctagggctgctcggg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>MR_7_1_MOS.sorted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            original                            closest  \\\n",
       "0  nnaacttcggtccgcaccctaggncngctcggg  ttaacttcggtccgcaccctagggctgctcggg   \n",
       "1  tnaacttcggtccgcaccctagggcngntcggg  ttaacttcggtccgcaccctagggctgctcggg   \n",
       "2  tcaacttcgnnccgcaccctagggcngctcggg  ttaacttcggtccgcaccctagggctgctcggg   \n",
       "3  tcaacttcggnccgcaccctagggcggctcnnn  tcaacttcggtccgcaccctagggctgctcggg   \n",
       "4  ttaacttcggtccgcaccctaggnctgctcggg  ttaacttcggtccgcaccctagggcggctcggg   \n",
       "5  ntaacttcggtccgcaccctagggctgntcggg  ttaacttcggtccgcaccctagggcggctcggg   \n",
       "6  tnaacttcggtccgcaccctaggnctgctcggg  ttaacttcggtccgcaccctagggctgctcggg   \n",
       "\n",
       "   distance               name  \n",
       "0       4.0    5_19_S_2.sorted  \n",
       "1       3.0      5_19_S.sorted  \n",
       "2       3.0      5_28_S.sorted  \n",
       "3       4.0      6_09_S.sorted  \n",
       "4       1.0  6_30_S_COL.sorted  \n",
       "5       2.0  6_30_S_MOS.sorted  \n",
       "6       2.0  MR_7_1_MOS.sorted  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_dis = pd.DataFrame(columns = [\"original\", \"closest\", \"distance\"])\n",
    "count = 0                       \n",
    "for seq in isms['seq'][:]:\n",
    "    new_temp = []\n",
    "    for t in new_thing[:]:\n",
    "        new_temp.append(lev_string(seq, t)[1])\n",
    "    t_array = [isms['name'][count], seq, thing[0][np.argmin(new_temp)], new_temp[np.argmin(new_temp)]]\n",
    "    seq_cur = pd.DataFrame([t_array], columns = [\"name\", \"original\", \"closest\", \"distance\"], index =[count])\n",
    "    seq_dis = seq_dis.append(seq_cur)\n",
    "    count += 1\n",
    "seq_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "may_19 = 'TTAACTTCGGTCCGCACCCTAGGGCGGCTCGGG'\n",
    "may_19 = may_19.lower()\n",
    "lev_string(may_19, new_thing[3])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you replace ambiguous bases with nonambiguous ones based on other ISMs from the wastewater project?\n",
    "\n",
    "Which region/country in the world does the cleaned ISMs found most abundant?\n",
    "\n",
    "These samples are collected in California in different dates. What are the most abundant subtype in California at those dates according to the ISM_df_with_correction.csv file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most abundant ISM(s) in California on 2020-05-19 00:00:00 is/are TCAACTTCGGTCCGCACCCTGGGGCGGCTTGGG\n",
      "The most abundant ISM(s) in California on 2020-05-28 00:00:00 is/are TCAACTTCGGTCCGCACCCTAGGGCGGCTCAAC\n",
      "The most abundant ISM(s) in California on 2020-06-09 00:00:00 is/are TCAACTTCGGTCCGCACCCTAGGGCGGCTCAAC\n",
      "The most abundant ISM(s) in California on 2020-06-30 00:00:00 is/are TCAACTTCGGTCCGCACCCTGGGGCGGCTTGGG, TTAACTTCGGTCCGCACCCTAGGGCTGTTCGGG\n",
      "The most abundant ISM(s) in California on 2020-07-01 00:00:00 is/are \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# gets the ISM that was most abundant in California on a specific date\n",
    "def get_most_abundant_ism_by_date(df, d):\n",
    "    # filter out unwanted columns and all entries not in california on the specified date\n",
    "    filtered_isms = df.loc[(df['date'] == d) & (df['division'] == 'California'), ['ISM', 'division', 'date']]\n",
    "    return filtered_isms['ISM'].mode() # return the ISM that appears the most\n",
    "\n",
    "# manually create dates\n",
    "dates = [datetime(2020, 5, 19),\n",
    "         datetime(2020, 5, 28),\n",
    "         datetime(2020, 6, 9),\n",
    "         datetime(2020, 6, 30),\n",
    "         datetime(2020, 7, 1)]\n",
    "\n",
    "ism_df = pd.read_excel('ISM_df_with_correction.xlsx')\n",
    "\n",
    "for d in dates:\n",
    "    d_seqs = get_most_abundant_ism_by_date(ism_df, d)\n",
    "    modes = \", \".join(d_seqs.array)\n",
    "    print(\"The most abundant ISM(s) in California on \" + str(d) + \" is/are \" + modes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\ry88\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.0.9)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\ry88\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ry88\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
